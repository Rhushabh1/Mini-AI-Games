{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: failed to set matplotlib backend, plotting will not work: No module named 'tkinter'\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers, logger\n",
    "from gym.utils.play import *\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space : Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Action space : Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"Observation space : {}\".format(env.observation_space))\n",
    "print(\"Action space : {}\".format(env.action_space))\n",
    "type(env.action_space)\n",
    "\n",
    "MAX_EPISODES = 30\n",
    "MAX_TRY = 10000\n",
    "total_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        action = self.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Clearing 10 monitor files from previous run (because force=True was provided)\n",
      "INFO: Starting new video recorder writing to /home/rhushabh/Desktop/Self - Projects/Mini-AI-Games/CartPole/random_agent_results/openaigym.video.0.233017.video000000.mp4\n",
      "INFO: Starting new video recorder writing to /home/rhushabh/Desktop/Self - Projects/Mini-AI-Games/CartPole/random_agent_results/openaigym.video.0.233017.video000001.mp4\n",
      "Episode:1\tLength:19\tReward:20.0\n",
      "Episode:2\tLength:21\tReward:22.0\n",
      "Episode:3\tLength:15\tReward:16.0\n",
      "Episode:4\tLength:27\tReward:28.0\n",
      "Episode:5\tLength:10\tReward:11.0\n",
      "Episode:6\tLength:21\tReward:22.0\n",
      "Episode:7\tLength:52\tReward:53.0\n",
      "INFO: Starting new video recorder writing to /home/rhushabh/Desktop/Self - Projects/Mini-AI-Games/CartPole/random_agent_results/openaigym.video.0.233017.video000008.mp4\n",
      "Episode:8\tLength:13\tReward:14.0\n",
      "Episode:9\tLength:16\tReward:17.0\n",
      "Episode:10\tLength:17\tReward:18.0\n",
      "Episode:11\tLength:16\tReward:17.0\n",
      "Episode:12\tLength:12\tReward:13.0\n",
      "Episode:13\tLength:51\tReward:52.0\n",
      "Episode:14\tLength:11\tReward:12.0\n",
      "Episode:15\tLength:31\tReward:32.0\n",
      "Episode:16\tLength:15\tReward:16.0\n",
      "Episode:17\tLength:11\tReward:12.0\n",
      "Episode:18\tLength:16\tReward:17.0\n",
      "Episode:19\tLength:12\tReward:13.0\n",
      "Episode:20\tLength:14\tReward:15.0\n",
      "Episode:21\tLength:18\tReward:19.0\n",
      "Episode:22\tLength:16\tReward:17.0\n",
      "Episode:23\tLength:11\tReward:12.0\n",
      "Episode:24\tLength:21\tReward:22.0\n",
      "Episode:25\tLength:10\tReward:11.0\n",
      "Episode:26\tLength:10\tReward:11.0\n",
      "INFO: Starting new video recorder writing to /home/rhushabh/Desktop/Self - Projects/Mini-AI-Games/CartPole/random_agent_results/openaigym.video.0.233017.video000027.mp4\n",
      "Episode:27\tLength:25\tReward:26.0\n",
      "Episode:28\tLength:22\tReward:23.0\n",
      "Episode:29\tLength:10\tReward:11.0\n",
      "Episode:30\tLength:12\tReward:13.0\n",
      "Average Scores:19.50\n",
      "INFO: Finished writing results. You can upload them to the scoreboard via gym.upload('/home/rhushabh/Desktop/Self - Projects/Mini-AI-Games/CartPole/random_agent_results')\n"
     ]
    }
   ],
   "source": [
    "logger.set_level(logger.INFO)\n",
    "outdir = \"./random_agent_results\"\n",
    "env = gym.wrappers.Monitor(env, directory = outdir, force = True)\n",
    "\n",
    "env.reset()\n",
    "agent = Agent(env)\n",
    "\n",
    "for i_episode in range(MAX_EPISODES):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for t in range(MAX_TRY):\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    total_scores.append(score)\n",
    "    print(\"Episode:{}\\tLength:{}\\tReward:{}\".format(i_episode+1, t, score))\n",
    "\n",
    "avg_score = sum(total_scores)/len(total_scores)\n",
    "print(\"Average Scores:{:.2f}\".format(avg_score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def writefile(fname, s):\n",
    "    with open(os.path.join(outdir, fname), 'w') as fh: fh.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writefile(\"agent.pkl\", str(pickle.dumps(agent, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD_TO_KEY = {\n",
    "    (ord('a'), ):0,\n",
    "    (ord('d'), ):1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, gym, time\n",
    "\n",
    "#\n",
    "# Test yourself as a learning agent! Pass environment name as a command-line argument, for example:\n",
    "#\n",
    "# python keyboard_agent.py SpaceInvadersNoFrameskip-v4\n",
    "#\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "if not hasattr(env.action_space, 'n'):\n",
    "    raise Exception('Keyboard agent only supports discrete action spaces')\n",
    "ACTIONS = env.action_space.n\n",
    "SKIP_CONTROL = 0    # Use previous control decision SKIP_CONTROL times, that's how you\n",
    "                    # can test what skip is still usable.\n",
    "\n",
    "human_agent_action = 0\n",
    "human_wants_restart = False\n",
    "human_sets_pause = False\n",
    "\n",
    "def key_press(key, mod):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    if key==0xff0d: human_wants_restart = True\n",
    "    if key==32: human_sets_pause = not human_sets_pause\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    human_agent_action = a\n",
    "\n",
    "def key_release(key, mod):\n",
    "    global human_agent_action\n",
    "    a = int( key - ord('0') )\n",
    "    if a <= 0 or a >= ACTIONS: return\n",
    "    if human_agent_action == a:\n",
    "        human_agent_action = 0\n",
    "\n",
    "env.render()\n",
    "env.unwrapped.viewer.window.on_key_press = key_press\n",
    "env.unwrapped.viewer.window.on_key_release = key_release\n",
    "\n",
    "def rollout(env):\n",
    "    global human_agent_action, human_wants_restart, human_sets_pause\n",
    "    human_wants_restart = False\n",
    "    obser = env.reset()\n",
    "    skip = 0\n",
    "    total_reward = 0\n",
    "    total_timesteps = 0\n",
    "    while 1:\n",
    "        if not skip:\n",
    "            #print(\"taking action {}\".format(human_agent_action))\n",
    "            a = human_agent_action\n",
    "            total_timesteps += 1\n",
    "            skip = SKIP_CONTROL\n",
    "        else:\n",
    "            skip -= 1\n",
    "\n",
    "        obser, r, done, info = env.step(a)\n",
    "        if r != 0:\n",
    "            print(\"reward %0.3f\" % r)\n",
    "        total_reward += r\n",
    "        window_still_open = env.render()\n",
    "        if window_still_open==False: return False\n",
    "        if done: break\n",
    "        if human_wants_restart: break\n",
    "        while human_sets_pause:\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "        time.sleep(0.1)\n",
    "    print(\"timesteps %i reward %0.2f\" % (total_timesteps, total_reward))\n",
    "\n",
    "print(\"ACTIONS={}\".format(ACTIONS))\n",
    "print(\"Press keys 1 2 3 ... to take actions 1 2 3 ...\")\n",
    "print(\"No keys pressed is taking action 0\")\n",
    "\n",
    "while 1:\n",
    "    window_still_open = rollout(env)\n",
    "    if window_still_open==False: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
